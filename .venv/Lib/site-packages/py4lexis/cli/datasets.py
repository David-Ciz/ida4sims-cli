from datetime import (
    date, 
    datetime
)
from logging import (
    Logger, 
    getLogger
)
from json import (
    dumps, 
    loads
)
from typing import (
    Generator, 
    Optional, 
    List
)
from enum import Enum

from pandas import DataFrame
# Making ASCII table
# Source: https://stackoverflow.com/questions/5909873/how-can-i-pretty-print-ascii-tables-with-python
from tabulate import tabulate
import typer
from typing_extensions import Annotated

from py4lexis.session import LexisSessionOffline
from py4lexis.ddi.datasets import Datasets
from py4lexis.core.directory_tree import DirectoryTree
from py4lexis.models.directory_tree import TreeDirectoryObject
from py4lexis.core.base.common import log_and_print_errors
from py4lexis.cli.typings import (
    Access,
    ActionType,
    UploadStatus,
    FilenameOperators,
    SizeOperators,
    ItemType
)


app = typer.Typer(name="datasets",
                  help="Holds commands to manage datasets in DDI. Type '[b]python -m py4lexis.cli datasets --help[/b]' to see list of available commands.",
                  rich_markup_mode="rich")


logger: Logger = getLogger(__name__)


@app.command()
def create_dataset(access: Annotated[
                        Access, 
                        typer.Argument(help="Access type used for the dataset.")
                   ],
                   project: Annotated[
                        str, 
                        typer.Argument(help="LEXIS Project's shortname.")
                   ], 
                   storage_name: Annotated[
                        str,
                        typer.Argument(help="iRODS storage location name where the dataset will be created.")
                   ],
                   storage_resource: Annotated[
                        str,
                        typer.Argument(help="iRODS storage resource that should be used to store the dataset. Value can be obtained from the LEXIS portal.")
                   ],                   
                   title: Annotated[
                        str,
                        typer.Option(help="Title of the dataset.")
                   ]=f"UNTITLED_Dataset_{datetime.now().strftime('%d-%m-%Y_%H:%M:%S')}",
                   additional_metadata: Annotated[
                        str,
                        typer.Option(help="Additional (arbitrary) metadata for the dataset in JSON format.")
                   ] = None,
                   dataset_type: Annotated[
                        str,
                        typer.Option(help="Type of the dataset. Default 'Dataset'.")
                   ] = None,
                   datacite: Annotated[
                        str,
                        typer.Option(help="Datacite metadata for the dataset in JSON format. DataCite object could be created within Py4Lexis.")
                   ] = None,
                   refresh_token: Annotated[
                       Optional[str],
                       typer.Option(help="Refresh (offline) token used for authentication. By default environemnt variable 'PY4LEXIS_TOKEN' is used.")
                   ]=None,
                   exit_on_error: Annotated[
                        bool,
                        typer.Option(help="If True, program will exit() on error.")
                   ]=True
                   ) -> None:
    """
    Creates an empty dataset with specified attributes.
    
    [bold blue]──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────[/bold blue]
    
    [bold yellow]Further description:[/bold yellow]
    
       [b yellow]*[/b yellow] It will print out an [b]dataset_id[/b] of the created dataset. Given ID should be used for other commands. 

    [bold blue]──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────[/bold blue]

    [bold red]Examples of usage[/bold red]:

    [b]Set --title: [/b]
        [b yellow]>[/b yellow] python3 -m py4lexis.cli.datasets create-dataset project demoproject "Demo Location" "Demo Resource" --title "Dataset from py4lexis CLI"

    [b]Use additionalMetadata and dataset_type. [/b]
        [b yellow]>[/b yellow] python3 -m py4lexis.cli.datasets create-dataset project demoproject "Demo Location" "Demo Resource" --additional-metadata '{\"aKey\" : \"aValue\", \"bKey\" : \"bValue\"}' --dataset-type 'dataset'
                                
    [b]Upload public dataset (Datacite model required): [/b]
        [b yellow]>[/b yellow] python3 -m py4lexis.cli.datasets create-dataset public demoproject "Demo Location" "Demo Resource" --datacite '{\"creators\": [{\"name\": \"User123\"}], \"titles\": [{\"lang\": \"en\", \"title\": \"Sample title\"}], \"publisher\": {\"name\": \"User123\"}, \"types\": {\"resourceType\": \"Dataset\", \"resourceTypeGeneral\": \"Dataset\"}, \"publicationYear\": 2024}'
    """
    
    session = LexisSessionOffline(in_cli=True,
                                  refresh_token=refresh_token,
                                  exit_on_error=exit_on_error)
    
    ds = Datasets(session=session,
                  suppress_print=False,
                  exit_on_error=exit_on_error)
    
    
    _ = ds.create_dataset(access=access.value, 
                          project=project, 
                          storage_name=storage_name,
                          storage_resource=storage_resource,                          
                          title=title,
                          dataset_type=dataset_type,
                          datacite=datacite,
                          additional_metadata=additional_metadata)
    
    return None


@app.command()
def tus_uploader_new(access: Annotated[
                         Access, 
                         typer.Argument(help="Access type used for the dataset.")
                     ],
                     project: Annotated[
                         str, 
                         typer.Argument(help="LEXIS project's shortname.")
                     ], 
                     filename: Annotated[
                         str, 
                         typer.Argument(help="Name of a file to be uploaded.")
                     ], 
                     storage_name: Annotated[
                        str,
                        typer.Argument(help="iRODS storage location name where the dataset will be created.")
                     ], 
                     storage_resource: Annotated[
                        str,
                        typer.Argument(help="iRODS storage resource that should be used to store the dataset. Value can be obtained from the LEXIS portal.")
                     ],                                         
                     file_path: Annotated[
                         Optional[str], 
                         typer.Option(help="Path to a file in user's machine. If [blue]None[/blue], then by default file_path='./'.")
                     ]=None,
                     dataset_type: Annotated[
                         str,
                         typer.Option(help="Type of the dataset. Default 'Dataset'.")
                     ] = None,
                     additional_metadata: Annotated[
                         str,
                         typer.Option(help="Additional metadata for the dataset in JSON format.")
                     ] = None,
                     datacite: Annotated[
                         str,
                         typer.Option(help="Datacite metadata for the dataset in JSON format.")
                     ] = None,
                     desination_path: Annotated[
                         str,
                         typer.Option(help="Relative path inside of the dataset.")
                     ] = "",
                     title:  Annotated[
                         str, 
                         typer.Option(help="Title of the dataset (by default untitled with datetime as ISO).")
                     ]=f"UNTITLED_Dataset_{datetime.now().strftime('%d-%m-%Y_%H:%M:%S')}",
                     refresh_token: Annotated[
                         Optional[str],
                         typer.Option(help="Refresh (offline) token used for authentication. By default environemnt variable 'PY4LEXIS_TOKEN' is used.")
                     ]=None,
                     exit_on_error: Annotated[
                         bool,
                         typer.Option(help="If True, program will exit() on error.")
                     ]=True
                    ) -> None:
    """
    Creates new dataset with provided metadata and performs TUS upload of a data file.
    
    [bold blue]──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────[/bold blue]
    
    [bold yellow]Further description:[/bold yellow]
    
       [b yellow]*[/b yellow] If [b].tar.gz[/b] or [b].zip[/b] file is uploaded, the datafiles will be extracted at server in automated way.

       [b yellow]*[/b yellow] [b]--file-path[/b] has default value (root directory of CWD). However, we recommend to always pass a value for a given option.

    [bold blue]──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────[/bold blue]

    [bold red]Examples of usage[/bold red]:

    [b]Set --file-path, --title and --desination_path : [/b]
        [b yellow]>[/b yellow] python -m py4lexis.cli.datasets tus-uploader-new project demoproject README.md "Demo Location" "Demo Resource"  --file-path ./  --title "CLI DATASET TEST" --desination-path input

    [b]Use of additionalMetadata, datacite, dataset_type[/b]
        [b yellow]>[/b yellow] See --help of dataset create command (parameters are passed by the same way). python3 -m py4lexis.cli.datasets create-dataset --help
        
    """
   
    session = LexisSessionOffline(in_cli=True,
                                  refresh_token=refresh_token,
                                  exit_on_error=exit_on_error)
    ds = Datasets(session=session,
                  suppress_print=False,
                  exit_on_error=exit_on_error)

    ds.tus_uploader_new(access=access.value, 
                        project=project, 
                        filename=filename, 
                        storage_resource=storage_resource,
                        storage_name=storage_name,
                        file_path=file_path,
                        dataset_type=dataset_type,
                        datacite=datacite,
                        additional_metadata=additional_metadata,
                        title=title,
                        desination_path=desination_path)
    return None


@app.command()
def tus_uploader_rewrite(dataset_id: Annotated[
                            str, 
                            typer.Argument(help="Dataset ID of an existing dataset (UUID)")
                         ],
                         project: Annotated[
                             str, 
                             typer.Argument(help="Project's shortname.")
                         ], 
                         filename: Annotated[
                             str, 
                             typer.Argument(help="Name of a file to be uploaded.")
                         ], 
                         storage_name: Annotated[
                            str,
                            typer.Argument(help="iRODS storage location name where the dataset will be created.")
                         ],
                         storage_resource: Annotated[
                            str,
                             typer.Argument(help="iRODS storage resource that should be used to store the dataset. Value can be obtained from the LEXIS portal.")
                         ],                         
                         file_path: Annotated[
                             Optional[str], 
                             typer.Option(help="Path to a file in user's machine. If [blue]None[/blue], then by default './'.")
                         ]=None,
                         destination_path: Annotated[
                            Optional[str],
                            typer.Option(help="Relative path inside of the dataset.")
                         ] = None,
                         refresh_token: Annotated[
                            Optional[str],
                            typer.Option(help="Refresh (offline) token used for authentication. By default environemnt variable 'PY4LEXIS_TOKEN' is used.")
                         ]=None,
                         exit_on_error: Annotated[
                           bool,
                           typer.Option(help="If True, program will exit() on error.")
                         ]=True) -> None:
    """
    Uploads a data file to an existing dataset identified by its [b]InternalID[/b].         
    
    [bold blue]──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────[/bold blue]
    
    [bold yellow]Further description:[/bold yellow]
    
       [b yellow]*[/b yellow] If [b].tar.gz[/b] or [b].zip[/b] file is uploaded, the datafiles will be extracted at server in automated way.

       [b yellow]*[/b yellow] [b]--file-path[/b] has default value (root directory of CWD). However, we recommend to always pass a value for a given option.

    [bold blue]──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────[/bold blue]

    [bold red]Examples of usage[/bold red]:

    [b]Simple rewrite of existing dataset: [/b]
        [b yellow]>[/b yellow] python -m py4lexis.cli.datasets tus-uploader-rewrite f80959da-a015-11ef-a399-0242ac1a000b  demoproject LICENSE ""Demo Location" Demo Resource" --file-path ./
    
    [b]Simple write with destination path: [/b]
        [b yellow]>[/b yellow] python -m py4lexis.cli.datasets tus-uploader-rewrite f80959da-a015-11ef-a399-0242ac1a000b  demoproject LICENSE "Demo Location"  "Demo Resource" --destination-path input
    """
    
    session = LexisSessionOffline(in_cli=True,
                                  refresh_token=refresh_token,
                                  exit_on_error=exit_on_error)
    ds = Datasets(session=session,
                  suppress_print=False,
                  exit_on_error=exit_on_error)

    ds.tus_uploader_rewrite(dataset_id=dataset_id,
                            project=project, 
                            filename=filename,
                            storage_resource=storage_resource,
                            storage_name=storage_name,
                            file_path=file_path,
                            desination_path=destination_path)
    
    return None


@app.command()
def get_dataset_status(filter_project: Annotated[
                            Optional[str], 
                            typer.Option(help="To filter records by LEXIS project's shortname.")
                        ]=None,
                        filter_action_type: Annotated[
                            Optional[ActionType], 
                            typer.Option(help="To filter records by transfer's action type.")
                        ]=None, 
                        refresh_token: Annotated[
                            Optional[str],
                            typer.Option(help="Refresh (offline) token used for authentication. By default environemnt variable 'PY4LEXIS_TOKEN' is used.")
                        ]=None,
                        exit_on_error: Annotated[
                           bool,
                           typer.Option(help="If True, program will exit() on error.")
                        ]=True
                       ) -> None:
    """
    Gets datasets upload status and prints all records in ASCII table.          
    
    [bold blue]──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────[/bold blue]
    
    [bold yellow]Further description:[/bold yellow]
    
       [b yellow]*[/b yellow] It is possible to use options --filter_project, and --filter_action_type to filter records.

    [bold blue]──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────[/bold blue]

    [bold red]Examples of usage[/bold red]:

    [b]Get upload status filtered by and --filter-project: [/b]
        [b yellow]>[/b yellow] python -m py4lexis.cli.datasets get-dataset-status --filter-project demoproject
    """
    session = LexisSessionOffline(in_cli=True,
                                  refresh_token=refresh_token,
                                  exit_on_error=exit_on_error)
    ds = Datasets(session=session,
                  suppress_print=False,
                  exit_on_error=exit_on_error)

    action_type: str | None = filter_action_type.value if filter_action_type is not None else None

    content: DataFrame = ds.get_dataset_status(project=filter_project,
                                               action_type=action_type, 
                                               content_as_pandas=True)

    try:
        print(f"Formatting pandas DataFrame into ASCII table...")
        
        cols: list[str] = ["project", "status", "result", "type"]
        datasets_table: DataFrame = content[cols]

        print(tabulate(datasets_table.values.tolist(), cols, tablefmt="grid"))

    except KeyError as kerr:
        log_and_print_errors(logger, f"Wrong or missing key '{kerr}' in response content as DataFrame!!!", False)
    
    return None


@app.command()
def get_all_datasets(filter_dataset_id: Annotated[
                         Optional[str], 
                         typer.Option(help="To filter records by dataset's UUID. If set, all others filter parameters are omitted.")
                     ]=None,
                     filter_title:  Annotated[
                         Optional[str], 
                         typer.Option(help="To filter records by dataset's title string/substring.")
                     ]=None,
                     filter_access:  Annotated[
                         Optional[Access], 
                         typer.Option(help="To filter records by access type.")
                     ]=None, 
                     filter_project: Annotated[
                         Optional[str], 
                         typer.Option(help="To filter records by project's short name.")
                     ]=None, 
                     filter_dataset_type: Annotated[
                         Optional[str],
                         typer.Option(help="To filter records by dataset type (arbitrary string).")
                     ]=None,
                     filter_zone:  Annotated[
                         Optional[str], 
                         typer.Option(help="To filter records by zone.")
                     ]=None,
                     refresh_token: Annotated[
                       Optional[str],
                       typer.Option(help="Refresh (offline) token used for authentication. By default environemnt variable 'PY4LEXIS_TOKEN' is used.")
                     ]=None,
                     exit_on_error: Annotated[
                           bool,
                           typer.Option(help="If True, program will exit() on error.")
                     ]=True
                    ) -> None:
    """
    Prints an ASCII table of the all existing datasets.          
    
    [bold blue]──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────[/bold blue]
    
    [bold yellow]Further description:[/bold yellow]
    
       [b yellow]*[/b yellow] It is possible to use options --filter_title, --filter_access, --filter_zone and --filter_project to filter records. 

    [bold blue]──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────[/bold blue]

    [bold red]Examples of usage[/bold red]:

    [b]Get all datasets and filter them by --filter-project and --filter-access: [/b]
        [b yellow]>[/b yellow] python -m py4lexis.cli.datasets get-all-datasets --filter-project demoproject --filter-access project 
    """
    session = LexisSessionOffline(in_cli=True,
                                  refresh_token=refresh_token,
                                  exit_on_error=exit_on_error)
    ds = Datasets(session=session,
                  suppress_print=False,
                  exit_on_error=exit_on_error)
    
    content: DataFrame = ds.get_all_datasets(dataset_id=filter_dataset_id,
                                             project=filter_project,
                                             access=filter_access,
                                             dataset_type=filter_dataset_type,
                                             title=filter_title,
                                             content_as_pandas=True)
    
    try:
        print(f"Formatting pandas DataFrame into ASCII table...")
        
        if "internalID" in list(content.columns):
            cols: list[str] = ["title", "access", "project_shortname", "zone", "internalID", "creation_date"]
        else:
            cols: list[str] = ["title", "access", "project_shortname", "zone", "dataset_id", "creation_date"]
        datasets_table: DataFrame = content[cols]

        if filter_zone is not None:
            datasets_table = datasets_table[datasets_table["zone"] == filter_zone]

        print(tabulate(datasets_table.values.tolist(), cols, tablefmt="grid"))

    except KeyError as kerr:
        log_and_print_errors(logger, f"Wrong or missing key '{kerr}' in response content as DataFrame!!!", False)
    
    return None


@app.command()
def delete_dataset_by_id(dataset_id: Annotated[
                             str, 
                             typer.Argument(help="Dataset ID of the dataset (UUID). Can be obtain by [blue]get_all_datasets()[/blue] method.")
                         ], 
                         target_path: Annotated[
                                Optional[str],
                                typer.Option(help="Relative path inside of the dataset. Specifies which dataset should be deleted.")
                         ]=None,                         
                         refresh_token: Annotated[
                            Optional[str],
                            typer.Option(help="Refresh (offline) token used for authentication. By default environemnt variable 'PY4LEXIS_TOKEN' is used.")
                         ]=None,
                         exit_on_error: Annotated[
                           bool,
                           typer.Option(help="If True, program will exit() on error.")
                         ]=True
                        ) -> None:
    """
    Deletes a dataset indentified by its [b]dataset ID[/b].

    [bold blue]──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────[/bold blue]
    
    [bold yellow]Further description:[/bold yellow]
    
       [b yellow]*[/b yellow] An exception will be thrown if passed dataset's dataset ID is not UUID. 

    [bold blue]──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────[/bold blue]

    [bold red]Examples of usage[/bold red]:

    [b]Remove whole dataset: [/b]
        [b yellow]>[/b yellow] python -m py4lexis.cli.datasets delete-dataset-by-id 266609c6-a017-11ef-a399-0242ac1a000b
    [b]Remove data i ndataset: [/b]
        [b yellow]>[/b yellow] python -m py4lexis.cli.datasets delete-dataset-by-id 266609c6-a017-11ef-a399-0242ac1a000b --target_path input
    """
    session = LexisSessionOffline(in_cli=True,
                                  refresh_token=refresh_token,
                                  exit_on_error=exit_on_error)
    ds = Datasets(session=session,
                  suppress_print=False,
                  exit_on_error=exit_on_error)

    _ = ds.delete_dataset_by_id(dataset_id=dataset_id, 
                                target_path=target_path)

    return None


@app.command()
def get_delete_status(request_id: Annotated[
                          str,
                          typer.Argument(help="Request ID of the delete request as UUID.")                        
                      ],
                      refresh_token: Annotated[
                          Optional[str],
                          typer.Option(help="Refresh (offline) token used for authentication. By default environemnt variable 'PY4LEXIS_TOKEN' is used.")
                      ]=None,
                      exit_on_error: Annotated[
                          bool,
                          typer.Option(help="If True, program will exit() on error.")
                      ]=True) -> None:
    """
    Get datasets' delete status information.

    [bold blue]──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────[/bold blue]
    
    [bold yellow]Further description:[/bold yellow]
    
       [b yellow]*[/b yellow] An exception will be thrown if passed request ID is not UUID. 

    [bold blue]──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────[/bold blue]

    [bold red]Examples of usage[/bold red]:

    [b]Get delete status of the request: [/b]
        [b yellow]>[/b yellow] python -m py4lexis.cli.datasets get-delete-status 266609c6-a017-11ef-a399-0242ac1a000b
    """
    session = LexisSessionOffline(in_cli=True,
                                  refresh_token=refresh_token,
                                  exit_on_error=exit_on_error)
    ds = Datasets(session=session,
                  suppress_print=False,
                  exit_on_error=exit_on_error)

    content: DataFrame = ds.get_dataset_status(request_id=request_id,
                                               content_as_pandas=True)
    
    try:
        print(f"Formatting pandas DataFrame into ASCII table...")
        
        if "internalID" in list(content.columns):
            cols: list[str] = ["title", "access", "project", "zone", "internalID", "creationDate"]
        else:
            cols: list[str] = ["title", "access", "project", "zone", "dataset_id", "creationDate"]
        
        datasets_table: DataFrame = content[cols]

        # TODO

        print(tabulate(datasets_table.values.tolist(), cols, tablefmt="grid"))

    except KeyError as kerr:
        log_and_print_errors(logger, f"Wrong or missing key '{kerr}' in response content as DataFrame!!!", False)

    return None

@app.command()
def download_dataset(dataset_id: Annotated[
                         str, 
                         typer.Argument(help="Dataset ID of an existing dataset (UUID).")
                     ],
                     path: Annotated[
                            Optional[str],
                            typer.Option(help="Relative path inside of the dataset.")
                     ]=None,                     
                     destination_filepath: Annotated[
                         str, 
                         typer.Option(help="Path (relative or absolute) for the destination file path where data will be downloaded, by default destination_filepath='./'")
                     ]="./",
                     refresh_token: Annotated[
                       Optional[str],
                       typer.Option(help="Refresh (offline) token used for authentication. By default environemnt variable 'PY4LEXIS_TOKEN' is used.")
                     ]=None,
                     exit_on_error: Annotated[
                           bool,
                           typer.Option(help="If True, program will exit() on error.")
                     ]=True
                    ) -> None:
    """
    Downloads dataset by a specified [b]dataset ID[/b].

    [bold blue]──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────[/bold blue]
    
    [bold yellow]Further description:[/bold yellow]
    
       [b yellow]*[/b yellow] It is possible to specify local desination file path. Default is set to: "./".

       [b yellow]*[/b yellow] Destination file will be the same as it is on the server. If path specifies the directory, then the destination will be a .zip file.

    [bold blue]──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────[/bold blue]

    [bold red]Examples of usage[/bold red]:

    [b]Download whole dataset: [/b]
        [b yellow]>[/b yellow] python -m py4lexis.cli.datasets download-dataset f6a81cec-9d00-11ef-91fd-f39c69bba647
        
    [b]Download folder from dataset: [/b]
        [b yellow]>[/b yellow] python -m py4lexis.cli.datasets download-dataset f6a81cec-9d00-11ef-91fd-f39c69bba647 --path input
    
    [b]Download file from the dataset: [/b]
        [b yellow]>[/b yellow] python -m py4lexis.cli.datasets download-dataset f6a81cec-9d00-11ef-91fd-f39c69bba647 --path input/data.txt
        
    [b]Download data specified by GLOB: [/b]
        [b yellow]>[/b yellow] python -m py4lexis.cli.datasets download-dataset f6a81cec-9d00-11ef-91fd-f39c69bba647 --path input/*.txt
        
    [b]Download file from the dataset to the specific location: [/b]
        [b yellow]>[/b yellow] python -m py4lexis.cli.datasets download-dataset f6a81cec-9d00-11ef-91fd-f39c69bba647 --path input/data.txt --destination-filepath ./output
    """
    session = LexisSessionOffline(in_cli=True,
                                  refresh_token=refresh_token,
                                  exit_on_error=exit_on_error)
    ds = Datasets(session=session,
                  suppress_print=False,
                  exit_on_error=exit_on_error)
    
    ds.download_dataset(dataset_id=dataset_id,
                        path=path,
                        destination_filepath=destination_filepath)
    return None
    

@app.command()
def get_content_of_dataset(dataset_id: Annotated[
                               str, 
                               typer.Argument(help="Dataset ID of an existing dataset (UUID).")
                           ], 
                           print_dir_tree: Annotated[
                               bool, 
                               typer.Option(help="If [blue]True[/blue], the directory tree within the dataset will be printed as ASCII Directory Tree. If [blue]False[/blue], records will be printed in ASCII table.")
                           ]=False,
                           filter_name: Annotated[
                               Optional[str], 
                               typer.Option(help="To filter content of the table by dir/file-name (only available for ASCII table content).")
                           ]=None, 
                           name_compare_type: Annotated[
                               FilenameOperators, 
                               typer.Option(help="""
                                               Type of comparison for filtering by name (only available for ASCII table content).
                                           
                                               
                                               * [green]'eq' -- equal[/green]: 'filter_dir/file-name [green]==[/green] table_dir/file-name',
                                           
                                               * [green]'in' -- within[/green]: 'filter_dir/file-name [green]in[/green] table_dir/file-name' (find all substring occurences).
                                           """)
                           ]=FilenameOperators.eq,
                           filter_itemtype: Annotated[
                               Optional[ItemType], 
                               typer.Option(help="To filter table of files by item type (only available for ASCII table content).")
                           ]=None, 
                           itemtype_compare_type: Annotated[
                               FilenameOperators, 
                               typer.Option(help="""
                                               Type of comparison for filtering by item type (only available for ASCII table content).
                                           
                                               
                                               * [green]'eq' -- equal[/green]: 'filter_itemtyp [green]==[/green] table_itemtype',
                                           
                                               * [green]'in' -- within[/green]: 'filter_itemtype [green]in[/green] table_itemtype' (find all substring occurences).
                                           """)
                           ]=FilenameOperators.eq,
                           filter_size: Annotated[
                               int, 
                               typer.Option(help="To filter table of files by size (only available for content as ASCII table). [green]Cannot be lower than 0![/green]")
                           ]=0,
                           size_compare_type: Annotated[
                               SizeOperators, 
                               typer.Option(help="""
                                               Type of comparison for filtering by size (only available for ASCII table content).
                                           
                                               * [green]'eq' -- equal[/green]: 'filter_size [green]==[/green] table_size',
                                           
                                               * [green]'lt' -- less than[/green]: 'filter_size [green]\u2039[/green] table_size',
                                           
                                               * [green]'leq' -- less or equal[/green]: 'filter_size [green]\u2039=[/green] table_size',
                                           
                                               * [green]'gt' -- grater than[/green]: 'filter_size [green]\u203A[/green] table_size',
                                           
                                               * [green]'geq' -- grater or equal[/green]: 'filter_size [green]\u203A=[/green] table_size'.
                                               """)
                           ]=SizeOperators.geq,
                           refresh_token: Annotated[
                             Optional[str],
                             typer.Option(help="Refresh (offline) token used for authentication. By default environemnt variable 'PY4LEXIS_TOKEN' is used.")
                           ]=None,
                           exit_on_error: Annotated[
                               bool,
                               typer.Option(help="If True, program will exit() on error.")
                           ]=True
                       ) -> None:
    """
    Get content of the dataset and prints them as Directory Tree or ASCII table.

    [bold blue]──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────[/bold blue]
    
    [bold yellow]Further description:[/bold yellow]
    
       [b yellow]*[/b yellow] If --print_dir_tree is used, then content will be printed as Directory Tree in ASCII.

       [b yellow]*[/b yellow] If --no-print-dir-tree option is used, then content will be printed as ASCII table.

       [b yellow]*[/b yellow] While considering content as ASCII table there is also possibility to use filter options: --filter-filename, --filter-itemtype, --filter-size. These options are not available while 
       considering the content as Directory Tree.

       [b yellow]*[/b yellow] While using --filter-filename there is possibility to define relation operator by --filename-compare-type.

       [b yellow]*[/b yellow] While using --filter-itemtype there is possibility to define relation operator by --itemtype-compare-type.

       [b yellow]*[/b yellow] While using --filter-size there is possibility to define relation operator by --filename-size-type.

    [bold blue]──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────[/bold blue]

    [bold red]Examples of usage[/bold red]:

    [b]Print files in dataset as Directory Tree: [/b]
        [b yellow]>[/b yellow] python -m py4lexis.cli.datasets get-content-of-dataset f6a81cec-9d00-11ef-91fd-f39c69bba647 --print-dir-tree

    [b]Print files in dataset as ASCII table: [/b]
        [b yellow]>[/b yellow] python -m py4lexis.cli.datasets get-content-of-dataset f6a81cec-9d00-11ef-91fd-f39c69bba647 --no-print-dir-tree
    
    [b]Print files in dataset as ASCII table and filter them by Dir/File-name with comparison as substring: [/b]
        [b yellow]>[/b yellow] python -m py4lexis.cli.datasets get-content-of-dataset f6a81cec-9d00-11ef-91fd-f39c69bba647 --no-print-dir-tree --filter-name _01 --name-compare-type in
    """
    session = LexisSessionOffline(in_cli=True,
                                  refresh_token=refresh_token,
                                  exit_on_error=exit_on_error)
    ds = Datasets(session=session,
                  suppress_print=False,
                  exit_on_error=exit_on_error) 

    content: list[dict] | DataFrame = ds.get_content_of_dataset(dataset_id=dataset_id,
                                                                content_as_pandas=not print_dir_tree)
                   
    if not print_dir_tree:
        try:
            print(f"Formatting pandas DataFrame into ASCII table...")
            
            cols: list[str] = ["Dir/File-name", "Path", "Type", "Size", "CreateTime", "Checksum"]
            datasets_table: DataFrame = content.copy()

            if filter_name is not None:
                if name_compare_type.value == "eq":
                    datasets_table = datasets_table[datasets_table["Dir/File-name"] == filter_name]
                elif name_compare_type.value == "in":
                    datasets_table = datasets_table[datasets_table["Dir/File-name"].str.contains(filter_name, case=False)]
                else:
                    print("Wrong comparison type for filename.")

            if filter_itemtype is not None:
                if itemtype_compare_type.value == "eq":
                    datasets_table = datasets_table[datasets_table["Type"] == filter_itemtype]
                elif itemtype_compare_type.value == "in":
                    datasets_table = datasets_table[datasets_table["Type"].str.contains(filter_itemtype, case=False)]
                else:
                    print("Wrong comparison type for itemtype.")

            if filter_size >= 0:
                if size_compare_type.value == "eq":
                    datasets_table = datasets_table[datasets_table["Size"] == filter_size]
                elif size_compare_type.value == "l":
                    datasets_table = datasets_table[datasets_table["Size"] < filter_size]
                elif size_compare_type.value == "leq":
                    datasets_table = datasets_table[datasets_table["Size"] <= filter_size]
                elif size_compare_type.value == "g":
                    datasets_table = datasets_table[datasets_table["Size"] > filter_size]
                elif size_compare_type.value == "geq":
                    datasets_table = datasets_table[datasets_table["Size"] >= filter_size]
                else:
                    print("Wrong comparison type for filesize.")
            else:
                print("Filesize cannot be negative.")

            print(tabulate(datasets_table.values.tolist(), cols, tablefmt="grid"))

        except KeyError as kerr:
            log_and_print_errors(logger, f"Wrong or missing key '{kerr}' in response content as DataFrame!!!", False)
    else:
        tree_content: TreeDirectoryObject = TreeDirectoryObject(content)
        tree_items: Generator[DirectoryTree, None, None] = DirectoryTree.make_tree(tree_content)
        for item in tree_items:
            print(item.to_string())
    
    return None


@app.command()
def get_dataset_path(access: Annotated[
                         Access, 
                         typer.Argument(help="Access type defined for dataset.")
                     ], 
                     project: Annotated[
                         str, 
                         typer.Argument(help="Project's shortname.")
                     ],  
                     dataset_id: Annotated[
                         str, 
                         typer.Argument(help="InternalID of an existing dataset (UUID).")
                     ], 
                     username: Annotated[
                         Optional[str], 
                         typer.Argument(help="The iRODS username. [green]Needed only when 'user' access is defined.[/green]")
                     ]=None,
                     refresh_token: Annotated[
                       Optional[str],
                       typer.Option(help="Refresh (offline) token used for authentication. By default environemnt variable 'PY4LEXIS_TOKEN' is used.")
                     ]=None,
                     exit_on_error: Annotated[
                          bool,
                          typer.Option(help="If True, program will exit() on error.")
                     ]=True
                    ) -> None:
    """
    Assembles and prints a path for an existing dataset as the combination of access, project, dataset ID (and username) argument.

    [bold blue]──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────[/bold blue]
    
    [bold yellow]Further description:[/bold yellow]
    
       [b yellow]*[/b yellow] --username is needed only when 'user' access is defined.

    [bold blue]──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────[/bold blue]

    [bold red]Examples of usage[/bold red]:

    [b]Get dataset path: [/b]
        [b yellow]>[/b yellow] python -m py4lexis.cli.datasets get-dataset-path project demoproject 7b55280a-b917-11ee-a79f-fa163e515f81
    """
    session = LexisSessionOffline(in_cli=True,
                                  refresh_token=refresh_token,
                                  exit_on_error=exit_on_error)
    ds = Datasets(session=session,
                  suppress_print=False,
                  exit_on_error=exit_on_error)
    
    path: str | None =  ds.get_dataset_path(access, project, dataset_id, username)

    if path is not None:
        print(f"Path: {path}")
    else:
        log_and_print_errors(logger, f"Some error occured while retrieving dataset's path. The path is '{None}'.\nInputs: {dumps({'access': access, 'project': project, 'dataset_id': dataset_id, 'username': username})}", False)


if __name__ == "__main__":
    app()