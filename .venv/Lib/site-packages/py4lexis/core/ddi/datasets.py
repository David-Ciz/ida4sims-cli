from __future__ import annotations
import os
from typing import Literal
from datetime import datetime
from logging import (
    Logger, 
    getLogger
)
from json import (
    dumps, 
    loads
)
from base64 import b64decode

from requests import Response
from tusclient import exceptions
from pandas import DataFrame

from py4lexis.core.ddi.tus_client import TusClient
from py4lexis.core.exceptions import (
    Py4LexisDownloadException, 
    Py4LexisTUSException,
    Py4LexisException
)
from py4lexis.session import (
    LexisSession,
    LexisSessionCredentials,
    LexisSessionOffline,
    LexisSessionURL
)
from py4lexis.core.utils import (
    convert_get_datasets_status_to_pandas,
    convert_dir_tree_to_pandas,
    convert_list_of_dicts_to_pandas, 
    printProgressBar, 
    assemble_dataset_path
)
from py4lexis.core.base.validators import (
    check_action_type,
    check_if_uuid,
    check_access,
    is_string,
    is_dictionary,
    is_bool
)
from py4lexis.core.base.paths import (
    make_tus_path,
    create_path,
    pure_path
)
from py4lexis.core.ddi.uploader import (
    AsyncUploader, 
    Uploader
)
from py4lexis.core.decorators import (
    handle_class_exceptions, 
    handle_common_variables
)

from py4lexis.core.typings.ddi import (
    Access,
    ActionType
)


class Datasets(object):
    def __init__(self,
                 session: LexisSession | LexisSessionURL | LexisSessionOffline | LexisSessionCredentials,
                 print_content: bool=False,
                 suppress_print: bool=True,
                 exit_on_error: bool=False) -> None:
        """
            A class holds methods to manage datasets within LEXIS platform.

            Attributes
            ----------
            session : LexisSession | LexisSessionURL | LexisSessionOffline | LexisSessionCredentials
                Class that holds LEXIS session
            print_content : bool | None
                If True then contents of all requests will be printed.
            suppress_print: bool | None
                If True then all other prints are suppressed. By default: suppress_print=True
            exit_on_error: bool
                If True, program will exit() on error.

            Methods
            -------
            create_dataset(access: Literal["public", "project", "user"], 
                           project: str, 
                           storage_name: str,
                           storage_resource: str,                       
                           dataset_type : str | None=None,
                           additional_metadata : dict | str | None=None,
                           datacite: dict | str | None=None,
                           title: str=str("UNTITLED_Dataset_" + datetime.now().strftime("%d-%m-%Y_%H:%M:%S"))) -> dict[str, str]
                Creates an empty dataset with specified attributes.

            tus_uploader_new(access: Literal["public", "project", "user"], 
                             project: str, 
                             filename: str,
                             storage_name: str, 
                             storage_resource: str,                         
                             file_path: str | None=None,
                             title: str="UNTITLED_TUS_Dataset_" + datetime.now().strftime("%d-%m-%Y_%H:%M:%S"),
                             dataset_type : str | None=None,
                             additional_metadata : dict | None=None,
                             datacite: dict | None=None,
                             desination_path: str="",
                             async_tus: bool=False) -> None
                Creates new dataset and performs TUS upload of a data file. If '.tar.gz' or '.zip' file is uploaded, the datafiles will be extracted at server in automated way.
                WARNING: Large files (above 10GB) should be uploaded via 'py4lexis.lexis_irods' methods. Uploading via TUS methods can behave in unexpected way.

            tus_uploader_rewrite(dataset_id: str,
                                 project: str, 
                                 filename: str,
                                 storage_name: str,
                                 storage_resource: str,                             
                                 file_path: str | None=None,
                                 desination_path: str="",
                                 async_tus: bool=False) -> None
                Uploads a data file to an existing dataset identified by its 'dataset_id'. If '.tar.gz' or '.zip' file is uploaded, the datafiles will be extracted at server in automated way.
                WARNING: Large files (above 10GB) should be uploaded via 'py4lexis.lexis_irods' methods. Uploading via TUS methods can behave in unexpected way.

            get_dataset_status(project: str | None=None,
                               action_type: str | None=None,
                               request_id: str | None=None,
                               content_as_pandas: bool=False) -> list[dict] | DataFrame
                Get datasets' transfer (stage, upload, download, delete) status information.

            get_all_datasets(self, 
                             dataset_id: str | None=None,
                             dataset_type: str | None=None,
                             project: str | None=None,
                             access: Access | None=None,
                             title: str | None=None,
                             content_as_pandas: bool=False) -> list[dict] | DataFrame
                Get all existing datasets records according to defined parameters.

            delete_dataset_by_id(dataset_id: str, 
                                 target_path: str | None=None) -> dict | None:
                Deletes a dataset by a specified dataset ID.

            download_dataset(self, 
                             dataset_id: str,
                             path: str = "",
                             destination_filepath: str="./") -> None:
                Downloads dataset by a specified dataset ID.
                It is possible to specify local desination directory. Default is set to = "./"

            get_content_of_dataset(self, 
                                   dataset_id: str, 
                                   content_as_pandas: bool=False) -> dict[str] | DataFrame:
                List all files within the dataset.

            get_dataset_path(access: Literal["public", "project", "user"], 
                             project: str, 
                             dataset_id: str, 
                             username: str | None=None) -> str | None
                Returns a path for an existing dataset as the combination of access, project, dataset ID and username.
        """
        
        self._exit_on_error: bool = exit_on_error
        self._logging: Logger = getLogger(__name__)
        self._print_content: bool = print_content
        self._suppress_print: bool = suppress_print
        self._default_zone: str = session.zone
        self.__session: LexisSession | LexisSessionURL | LexisSessionOffline | LexisSessionCredentials = session        
    

    @handle_class_exceptions()
    @handle_common_variables()
    def create_dataset(self, 
                       access: Access, 
                       project: str, 
                       storage_name: str,
                       storage_resource: str,                       
                       dataset_type : str | None=None,
                       additional_metadata : dict | str | None=None,
                       datacite: dict | str | None=None,
                       title: str=str("UNTITLED_Dataset_" + datetime.now().strftime("%d-%m-%Y_%H:%M:%S"))) -> dict[str, str]:
        """
            Creates an empty dataset with specified attributes

            Parameters
            ----------
            access : Literal["public", "project", "user"], 
                Access type for the dataset.
            project: str, 
                LEXIS project's shortname.
            storage_name: str,
                iRODS storage location name where the dataset will be created.
            storage_resource: str,
                iRODS storage resource that should be used to store the dataset. 
                Value can be obtained from the LEXIS portal.
            dataset_type : str | None=None,
                Type of the dataset. Example value can be, e.g., 'dataset'.
            additional_metadata : dict | str | None=None,
                Additional (arbitrary) metadata of the dataset. It should be keyword dictionary, 
                where key is the metadata name and value is the metadata value. Could be 
                also passed as JSON string.
            datacite: dict | str | None=None,
                Datacite metadata of the dataset. It should correspond to the Datacite schema,
                which can be be generated by DataCite model within Py4Lexis.
                Could be also passed as JSON string.
            title: str=str("UNTITLED_Dataset_" + datetime.now().strftime("%d-%m-%Y_%H:%M:%S"))
                Title of the dataset.           

            Returns
            -------
            dict[str, str]
                Content of the HTTP request as JSON.
        """

        access = check_access(access=access,
                              log=self._logging)

        project = is_string(parameter_name="project",
                            parameter=project,
                            log=self._logging)
        
        storage_name = is_string(parameter_name="storage_name",
                                 parameter=storage_name,
                                 log=self._logging)
        
        storage_resource = is_string(parameter_name="storage_resource",
                                     parameter=storage_resource,
                                     log=self._logging)
        
        title = is_string(parameter_name="title",
                          parameter=title,
                          log=self._logging)
        
        dataset_type = is_string(parameter_name="dataset_type",
                                 parameter=dataset_type,
                                 log=self._logging)
        
        additional_metadata = is_dictionary(parameter_name="additional_metadata",
                                            parameter=additional_metadata,
                                            log=self._logging)
        
        datacite = is_dictionary(parameter_name="datacite",
                                 parameter=datacite,
                                 log=self._logging)

        post_body: dict = {
            "access": access,
            "project": project,
            "target_system": storage_name,
            "target_resource": storage_resource,
            "title": title
        }
        
        if dataset_type is not None:
            post_body["dataset_type"] = dataset_type

        if additional_metadata is not None:
            post_body["additionalMetadata"] = additional_metadata
            
        if access == "public" and datacite is None:
            raise Py4LexisException("Datacite metadata is required for public datasets.")
        
        if datacite is not None:
            post_body["datacite"] = datacite
        
        if not self._suppress_print:
            print(f"Creating dataset\n" + dumps(post_body, indent=4) + "\n")

        url: str = self.__session.ddi_path + b64decode(b"L21ldGEvZGF0YXNldA===").decode("utf-8")
        content: dict = self.__session.make_request("post", 
                                                    url, 
                                                    json_data=post_body, 
                                                    headers=self.__session.get_api_headers(),
                                                    to_json=True)
    
        if not self._suppress_print:
            print(f"The dataset was successfully created with dataset ID: {content['dataset_id']}")

        if self._print_content:
            print(f"Content:\n{content}")
        
        return {"dataset_id": content["dataset_id"]}
    

    @handle_class_exceptions()
    @handle_common_variables()
    def tus_uploader_new(self, 
                         access: Access, 
                         project: str, 
                         filename: str,
                         storage_name: str, 
                         storage_resource: str,                         
                         file_path: str | None=None,
                         title: str="UNTITLED_TUS_Dataset_" + datetime.now().strftime("%d-%m-%Y_%H:%M:%S"),
                         dataset_type : str | None=None,
                         additional_metadata : dict | None=None,
                         datacite: dict | None=None,
                         desination_path: str="",
                         async_tus: bool=False) -> None:
        """
            Creates new dataset and performs TUS upload of a data file. If '.tar.gz' or '.zip' file is uploaded, the datafiles will be extracted at server in automated way.

            WARNING: Large files (above 10GB) should be uploaded via 'py4lexis.lexis_irods' methods. Uploading via TUS methods can behave in unexpected way.

            Parameters
            ----------
            access : Literal["public", "project", "user"], 
                Access type for the dataset.
            project: str, 
                LEXIS project's shortname.
            filename: str
                Name of a file to be uploaded (with extension).
            file_path: str | None,
                Path to a file in user's machine. If None, then by default "./".
            storage_name: str,
                Location where the dataset will be created.
            storage_resource: str,
                Name of resource that should be used to store the dataset. Value can be gained from the LEXIS portal.            
            dataset_type : str | None=None,
                Type of the dataset (by default None). Example value can be 'dataset'.
            additional_metadata : dict | None=None,
                Additional metadata of the dataset. It should be keyword dictionary, 
                where key is the metadata name and value is the metadata value.
            datacite: dict | None=None,
                Datacite metadata. It should correspond to the Datacite schema.
                In Py4Lexis, it can be generated by DataCite model.
            title: str
                Title of the dataset (by default untitled with datetime as ISO).
            desination_path: str
                Relative path inside of the dataset. Place where data will be stored.
            async_tus: bool
                If TUS AsyncUploader should be used or not. By default it is set to False.

            Returns
            -------
            None
        """

        access = check_access(access=access,
                              log=self._logging)
        
        project = is_string(parameter_name="project",
                            parameter=project,
                            log=self._logging)
        
        filename = is_string(parameter_name="filename",
                             parameter=filename,
                             log=self._logging)
        
        storage_name = is_string(parameter_name="storage_name",
                                 parameter=storage_name,
                                 log=self._logging) 
        
        storage_resource = is_string(parameter_name="storage_resource",
                                     parameter=storage_resource,
                                     log=self._logging)                     
        
        file_path = is_string(parameter_name="file_path",
                              parameter=file_path,
                              log=self._logging)
        
        title = is_string(parameter_name="title",
                          parameter=title,
                          log=self._logging)
        
        dataset_type = is_string(parameter_name="dataset_type",
                                 parameter=dataset_type,
                                 log=self._logging)
        
        additional_metadata = is_dictionary(parameter_name="additional_metadata",
                                            parameter=additional_metadata,
                                            log=self._logging)
        
        datacite = is_dictionary(parameter_name="datacite",
                                 parameter=datacite,
                                 log=self._logging)
        
        desination_path = is_string(parameter_name="desination_path",
                                    parameter=desination_path,
                                    log=self._logging)
        
        async_tus = is_bool(parameter_name="async_tus",
                            parameter=async_tus,
                            log=self._logging)

        file_path = make_tus_path(file_path=file_path, 
                                  filename=filename)
        
        is_expand: str = "yes" if (".tar.gz" in file_path or ".zip" in file_path) else "no"
        
        metadata: dict = {
            "path": desination_path,
            "filename": os.path.basename(file_path),
            "expand": is_expand,
            "project": project,
            "access": access,
            "target_system": storage_name,
            "target_resource": storage_resource,
            "title": title
        }

        if dataset_type is not None:
            metadata["dataset_type"] = dataset_type

        if additional_metadata is not None:
            metadata["additionalMetadata"] = dumps(additional_metadata)
            
        if access == "public" and datacite is None:
            raise Py4LexisException("Datacite metadata is required for public datasets.")
        
        if datacite is not None:
            metadata["datacite"] = dumps(datacite)
        
        self.__tus_upload(file_path=file_path, 
                          metadata=metadata, 
                          async_tus=async_tus)
        return None
    

    @handle_class_exceptions()
    @handle_common_variables()
    def tus_uploader_rewrite(self, 
                             dataset_id: str,
                             project: str, 
                             filename: str,
                             storage_name: str,
                             storage_resource: str,                             
                             file_path: str | None=None,
                             desination_path: str="",
                             async_tus: bool=False) -> None:
        """
            Uploads a data file to an existing dataset identified by its 'dataset_id'. If '.tar.gz' or '.zip' file is uploaded, the datafiles will be extracted at server in automated way.

            WARNING: Large files (above 10GB) should be uploaded via 'py4lexis.lexis_irods' methods. Uploading via TUS methods can behave in unexpected way.

            Parameters
            ----------
            dataset_id : str
                Internal ID of existing dataset (UUID).
            project: str
                LEXIS project's shortname in which dataset is stored.
            filename: str
                Name of a file to be uploaded.
            storage_name: str,
                Location where the dataset will be created.
            storage_resource: str,
                Name of resource that should be used to store the dataset. Value can be gained from the LEXIS portal.            
            file_path: str | None,
                Path to a file in user's machine. If None, then by default "./".
            desination_path: str
                Relative path inside of the dataset. Place where data will be stored.
            async_tus: bool
                If TUS AsyncUploader should be used or not. By default it is set to False.

            Returns
            -------
            None
        """
        
        dataset_id = is_string(parameter_name="dataset_id",
                               parameter=dataset_id,
                               log=self._logging)
        
        project = is_string(parameter_name="project",
                            parameter=project,
                            log=self._logging)
        
        filename = is_string(parameter_name="filename",
                             parameter=filename,
                             log=self._logging)
        
        storage_name = is_string(parameter_name="storage_name",
                                 parameter=storage_name,
                                 log=self._logging) 
        
        storage_resource = is_string(parameter_name="storage_resource",
                                     parameter=storage_resource,
                                     log=self._logging)                     
        
        file_path = is_string(parameter_name="file_path",
                              parameter=file_path,
                              log=self._logging)
        
        desination_path = is_string(parameter_name="desination_path",
                                    parameter=desination_path,
                                    log=self._logging)
        
        async_tus = is_bool(parameter_name="async_tus",
                            parameter=async_tus,
                            log=self._logging)

        file_path = make_tus_path(file_path=file_path, 
                                  filename=filename)
        
        is_expand: bool = "yes" if (".tar.gz" in file_path or ".zip" in file_path) else "no"

        metadata: dict = {
            "path": desination_path,
            "filename": os.path.basename(file_path),
            "expand": is_expand,
            "project": project,
            "target_system": storage_name,
            "target_resource": storage_resource,
            "dataset_id": dataset_id
        }

        self.__tus_upload(file_path=file_path, 
                          metadata=metadata, 
                          async_tus=async_tus)
        
        return None


    def __tus_upload(self, 
                     file_path: str, 
                     metadata: dict, 
                     async_tus: bool=False) -> None:
        
        if not self._suppress_print:
            if "dataset_id" in metadata.keys():
                print(f"Initialising TUS upload to an existing dataset\n"+
                      f"    dataset_id: {metadata.get('dataset_id', None)}")
            else:
                print(f"Initialising TUS upload to a new dataset")
                  
            print(f"    project: { metadata.get('project', '') },\n"+
                  f"    access: { metadata.get('access', '') },\n"+
                  f"    resource: { metadata.get('target_resource', '')},\n"+
                  f"    target location: { metadata.get('target_system', '')},\n"+
                  f"with following metadata:")
            print(dumps(loads(metadata.get("datacite", "{}")), indent=4))
            
        if os.path.getsize(file_path) > 10e9:
            print(f"WARNING: Large files (above 10GB) should be uploaded via 'py4lexis.lexis_irods' methods.\n"+
                  f"         Uploading via TUS methods can behave in unexpected way. ")

        url: str = self.__session.ddi_path + b64decode(b'L3RyYW5zZmVyL3VwbG9hZC8=').decode("utf-8")

        try:
            tus_client: TusClient = TusClient(url,
                                              headers=self.__session.get_api_headers(add_content_type=False))
            
            if not self._suppress_print:
                print(f"Initialising TUS uploader...")

            if not async_tus:
                uploader: Uploader = tus_client.uploader(file_path=file_path, 
                                                         chunk_size=1048576,
                                                         metadata=metadata)
            else:
                uploader: AsyncUploader = tus_client.async_uploader(file_path=file_path, 
                                                                    chunk_size=1048576,
                                                                    metadata=metadata)
            
            uploader.upload()
            return None

        except exceptions.TusCommunicationError as te:
            raise Py4LexisTUSException(f"{te.response_content}")


    @handle_class_exceptions()
    def get_dataset_status(self, 
                           project: str | None=None,
                           action_type: ActionType | None=None,
                           request_id: str | None=None,
                           content_as_pandas: bool=False) -> list[dict] | DataFrame:
        """
            Get datasets' transfer (stage, upload, download, delete) status information.

            Parameters
            ----------
            project : str | None
                If not 'None', returns all records according to defined LEXIS project shortname. If 'request_id' is defined, 'lexis_project_shortname' is ignored.
            action_type : Literal["stage", "upload", "download", "delete"] | None
                If not 'None', returns all records according to defined action type. If 'request_id' is defined, 'action_type' is ignored.
            request_id : str | None
                If 'None', returns record based on its 'request_id'. Parameters 'lexis_project_shortname' and 'action_type' are ignored. 
            content_as_pandas : bool | None
                Convert HTTP response content from JSON to pandas DataFrame. By default: content_as_pandas=False

            Returns
            -------
            list[dict] | DataFrame
                Content of the HTTP request as list of JSONs dictionaries or pandas DataFrame if 'content_as_pandas=True'. 
        """
    
        project = is_string(parameter_name="project",
                            parameter=project,
                            log=self._logging)
        
        action_type = check_action_type(action_type=action_type,
                                        log=self._logging)
        
        request_id = check_if_uuid(uuid=request_id, 
                                   parameter_name="request_id",
                                   log=self._logging)  

        content_as_pandas = is_bool(parameter_name="content_as_pandas",
                                    parameter=content_as_pandas,
                                    log=self._logging)      

        if not self._suppress_print:
            print(f"Retrieving upload status of the datasets...")

        url: str = self.__session.ddi_path + b64decode(b"L3RyYW5zZmVyL3N0YXR1cw==").decode("utf-8")
        
        if request_id is not None:
            url = url + f"/{request_id}"

        if request_id is None and project is not None:
            if "=" in url:
                url = url + f"&project={project}"
            else:
                url = url + f"?project={project}"

        if request_id is None and action_type is not None:
            if "=" in url:
                url = url + f"&action_type={action_type}"
            else:
                url = url + f"?action_type={action_type}"

        all_records: list[dict] = []

        if request_id is not None:
            content: dict = self.__session.make_request("get",
                                                        url,
                                                        headers=self.__session.get_api_headers(),
                                                        to_json=True)
            
            all_records.append(content)
        else:
            start: int = 0
            tmp_url: str = ""
            is_all: bool = False

            while not is_all:
                if "=" in url:
                    tmp_url = url + f"&start={start}"
                else:
                    tmp_url = url + f"?start={start}"
                    
                content: dict = self.__session.make_request("get",
                                                            tmp_url,
                                                            headers=self.__session.get_api_headers(),
                                                            to_json=True)

                content_length: int = content.get("total", None)
                records: list[dict] = content.get("requests", None)

                if content_length is None:
                    raise Py4LexisException(f"Cannot get all datasets' status records!!! Returned size of records is 'None'!!!")
                
                if records is None:
                    raise Py4LexisException(f"Cannot get all datasets' status records!!! Returned records are 'None'!!!")
                
                current_length: int = len(records)

                all_records.extend(records)
                if start < content_length:
                    start = start + current_length
                else:
                    is_all = True

        if content_as_pandas:
            all_records: DataFrame = convert_list_of_dicts_to_pandas(session=self.__session,
                                                                     content=all_records, 
                                                                     suppress_print=self._suppress_print)

        if not self._suppress_print:
            print("Upload status of datasets successfully retrieved (and converted).") 

        if self._print_content:
            print(f"Content: {all_records}")
        
        return all_records


    @handle_class_exceptions()
    def get_all_datasets(self, 
                         dataset_id: str | None=None,
                         dataset_type: str | None=None,
                         project: str | None=None,
                         access: Access | None=None,
                         title: str | None=None,
                         content_as_pandas: bool=False) -> list[dict] | DataFrame:
        """
            Get all existing datasets records according to defined parameters.

            Parameters
            ----------
            dataset_id: str | None
                UUID of existing dataset. If set, other parameters (filters) are omitted. A single record corresponding to UUID is returned.
            dataset_type: str | None
                To filter datasets records by 'dataset_type' parameter. Could be an arbitrary string.
            project: str | None
                To filter datasets records by LEXIS project shortname.
            access: Literal["public", "project", "user"] | None
                To filter datasets records by access type.
            title: str | None
                To filter datasets records by dataset's title. It can be only part of the title, then all records matching title string are returned.
            content_as_pandas: bool | None
                Convert HTTP response content from JSON to pandas DataFrame. By default: content_as_pandas=False

            Returns
            -------
            list[dict] | DataFrame 
                Content of the HTTP request as JSON or as pandas DataFrame if 'content_as_pandas=True'.
        """
        dataset_id = check_if_uuid(uuid=dataset_id,
                                   parameter_name="dataset_id",
                                   log=self._logging)

        dataset_type = is_string(parameter_name="dataset_type",
                                 parameter=dataset_type,
                                 log=self._logging)
        
        project = is_string(parameter_name="project",
                            parameter=project,
                            log=self._logging)
        
        if access is not None:
            access = check_access(access=access,
                                log=self._logging)

        title = is_string(parameter=title,
                          parameter_name="title",
                          log=self._logging)

        content_as_pandas = is_bool(parameter_name="content_as_pandas",
                                    parameter=content_as_pandas,
                                    log=self._logging)

        if not self._suppress_print:
            print(f"Retrieving data of the datasets...")

        url: str = self.__session.ddi_path + b64decode(b'L21ldGEvc2VhcmNo').decode("utf-8")
        
        all_records: list[dict] = []

        if dataset_id is not None:
            url = url + f"/{dataset_id}"
            content: dict = self.__session.make_request("get",
                                                        url,
                                                        headers=self.__session.get_api_headers(),
                                                        to_json=True)
            
            all_records.append(content["lexis"])
        else:
            start: int = 0
            tmp_url: str = ""
            is_all: bool = False

            req_body: dict[str, dict] = { "fields": {} }
            if title is not None:
                req_body["fields"].update({"title": title})

            if dataset_type is not None:
                if "=" in url:
                    url = url + f"&dataset_type={dataset_type}"
                else:
                    url = url + f"?dataset_type={dataset_type}"

            if project is not None:
                if "=" in url:
                    url = url + f"&project={project}"
                else:
                    url = url + f"?project={project}"

            if access is not None:
                if "=" in url:
                    url = url + f"&access={access}"
                else:
                    url = url + f"?access={access}"

            while not is_all:
                if "=" in url:
                    tmp_url = url + f"&start={start}"
                else:
                    tmp_url = url + f"?start={start}"
                    
                content: dict = self.__session.make_request("post",
                                                            tmp_url,
                                                            headers=self.__session.get_api_headers(),
                                                            json_data=req_body,
                                                            to_json=True)

                content_length: int = content.get("total", None)
                records: list[dict] = content.get("datasets", None)

                if content_length is None:
                    raise Py4LexisException(f"Cannot get all datasets' status records!!! Returned size of records is 'None'!!!")
                
                if records is None:
                    raise Py4LexisException(f"Cannot get all datasets' status records!!! Returned records are 'None'!!!")
                
                current_length: int = len(records)

                for record in records:
                    all_records.append(record["lexis"])

                if start < content_length:
                    start = start + current_length
                else:
                    is_all = True
        
        if content_as_pandas:
            all_records: DataFrame = convert_list_of_dicts_to_pandas(session=self.__session,
                                                                     content=all_records, 
                                                                     suppress_print=self._suppress_print)
                                
        if not self._suppress_print:
            print(f"Data of the datasets successfully retrieved (and converted)....")

        if self._print_content:
            print(f"Content: {all_records}")
    
        return all_records


    @handle_class_exceptions()
    @handle_common_variables()
    def delete_dataset_by_id(self, 
                             dataset_id: str, 
                             target_path: str | None = None) -> dict | None:
        """
            Deletes a dataset by a specified dataset ID.

            Parameters
            ----------
            dataset_id : str
                Dataset ID of the dataset (UUID). Can be obtain by get_all_datasets() method.
            target_path : str | None
                Path which specifies which data should be deleted. If None, whole dataset will be deleted.
                If any existing data (folder) is specified, only that data will be deleted.

            Returns
            -------
            dict | None
                Content of the HTTP request.
        """
        
        dataset_id = check_if_uuid(uuid=dataset_id,
                                   parameter_name="dataset_id",
                                   log=self._logging)
        
        target_path = is_string(parameter_name="target_path",
                                parameter=target_path,
                                log=self._logging)

        url: str = self.__session.ddi_path + b64decode(b"L3N0YWdpbmcvZGVsZXRl").decode("utf-8")
        
        delete_body: dict[str, str] = {
            "dataset_id": dataset_id
        }

        if target_path is not None:
            delete_body["path"] = target_path
        
        content: dict = self.__session.make_request("delete",
                                                    url,
                                                    json_data=delete_body,
                                                    headers=self.__session.get_api_headers(),
                                                    to_json=True)
    
        if not self._suppress_print:
            print(f"Request to delete the dataset with dataset ID '{dataset_id}' has been sent...")

        if self._print_content:
            print(f"Content: {content}")
        
        return content
    

    @handle_class_exceptions()
    def get_delete_status(self, 
                          request_id: str | None = None,
                          content_as_pandas: bool=False) -> list[dict] | DataFrame:
        """
            Get datasets' delete status information.

            Parameters
            ----------
            request_id : str | None
                Request ID of the delete request as UUID.
            content_as_pandas : bool | None
                Convert HTTP response content from JSON to pandas DataFrame. By default: content_as_pandas=False.

            Returns
            -------
            list[dict] | DataFrame
                Content of the HTTP request as list of JSONs dictionaries or pandas DataFrame if 'content_as_pandas=True'. 
        """
        request_id = check_if_uuid(uuid=request_id,
                                   parameter_name="request_id",
                                   log=self._logging)
        
        content_as_pandas = is_bool(parameter=content_as_pandas,
                                    parameter_name="content_as_pandas",
                                    log=self._logging)
        
        if not self._suppress_print:
            print(f"Retrieving delete status of the datasets...")

        url: str = self.__session.ddi_path + b64decode(b'L3N0YWdpbmcvc3RhdHVz').decode("utf-8")
        
        if request_id is not None:            
            url = url + f"/{request_id}"
        else:
            url = f"{url}?action_type=delete"

        content: list[dict] = self.__session.make_request("get",
                                                          url,
                                                          headers=self.__session.get_api_headers(),
                                                          to_json=True)

        if content_as_pandas:
            if request_id is not None:
                content = [content]
            content: DataFrame = convert_get_datasets_status_to_pandas(content, 
                                                                       suppress_print=self._suppress_print)

        if not self._suppress_print:
            print("Delete status of datasets successfully retrieved (and converted).") 

        if self._print_content:
            print(f"Content: {content}")
        
        return content
          

    @handle_class_exceptions()
    @handle_common_variables()
    def download_dataset(self, 
                         dataset_id: str,
                         path: str = "",
                         destination_filepath: str="./") -> None:
        """
            Downloads dataset by a specified dataset ID.
            It is possible to specify local desination directory. Default is set to = "./"
            Data are downloaded and zippend on tthe fly on the server side. Client downloads the zip file.
            
            Parameters
            ----------
            dataset_id: str
                DatasetID of the dataset (UUID). Can be obtain by get_all_datasets() method.
            path: str
                Path to the data (folder) within the dataset. By default: "".
            destination_filepath: str | None
                Path (relative or absolute) to the local destination filepath without file name. File name is generated automatically using dataset_id.

            Returns
            -------
            None
        """

        dataset_id = check_if_uuid(uuid=dataset_id,
                                   parameter_name="dataset_id",
                                   log=self._logging)
        
        path = is_string(parameter=path,
                         parameter_name="path",
                         log=self._logging)
        
        destination_filepath = is_string(parameter=destination_filepath,
                                         parameter_name="destination_filepath",
                                         log=self._logging)
        
        destination_filepath = create_path(destination_filepath, is_dir=True)

        if not self._suppress_print:
            progress_func=printProgressBar
        else:
            progress_func=None
            
            
        url: str = self.__session.ddi_path + b64decode(b'L3RyYW5zZmVyL3ppcA==').decode("utf-8")
        body = {
            "dataset_id": dataset_id
        }

        if path:
            body["path"] = path
        else:
            body["path"] = ""

        print("STARTING DOWNLOAD")
        response: Response = self.__session.make_request("post",
                                                         url,
                                                         json_data=body,
                                                         headers=self.__session.get_api_headers(),
                                                         stream=True,
                                                         return_response=True)
        
        try:
            # File ready, start downloading
            total_length = response.headers.get("Length-of-data")
            file_name = response.headers.get("content-disposition").split("filename=")[1].strip('/')
            destination_filepath = pure_path(os.path.join(destination_filepath, file_name))
            self.__session._logging.info("STARTING DOWNLOAD")

            with open(destination_filepath, "wb") as f:
                total_length : int = int(total_length)
                current: int = int(0)
                chunk_size: int = int(32768)
            
                for data in response.iter_content(chunk_size=chunk_size):
                    if progress_func:
                        progress_func(current, 
                                        total_length, 
                                        prefix='Progress: ', 
                                        suffix='Downloaded', 
                                        length=50)
                        current += len(data)
                    f.write(data)
                if progress_func:
                    progress_func(total_length, 
                                    total_length, 
                                    prefix='Progress: ', 
                                    suffix='Downloaded', 
                                    length=50)

        except KeyError as kerr:
            raise Py4LexisDownloadException(f"Download ERROR -- Wrong or missing key '{str(kerr)}' in response.", url=url)

        except IOError as ioe:
            raise Py4LexisDownloadException(f"Download ERROR -- IOError: '{str(ioe)}' in response.", url=url)
                    
        if not self._suppress_print:
            print("Dataset successfully downloaded -- OK!")
            print(f"Dataset downloaded to: {destination_filepath}")

        return None
      

    @handle_class_exceptions()
    @handle_common_variables()
    def get_content_of_dataset(self, 
                               dataset_id: str, 
                               content_as_pandas: bool=False) -> dict[str] | DataFrame:
        """
            List all files within the dataset.

            Parameters
            ----------
            dataset_id : str
                Dataset ID of the dataset. Can be obtain by get_all_datasets() method.
            content_as_pandas : bool | None
                Convert HTTP response content from JSON to pandas DataFrame. By default: content_as_pandas=False

            Returns
            -------
            list[dict] | DataFrame
                Content of the HTTP request as JSON or as pandas DataFrame if 'content_as_pandas=True'.
        """

        dataset_id = check_if_uuid(uuid=dataset_id,
                                   parameter_name="dataset_id",
                                   log=self._logging)
        
        content_as_pandas = is_bool(parameter=content_as_pandas,
                                    parameter_name="content_as_pandas",
                                    log=self._logging)

        if not self._suppress_print:
            print(f"Retrieving data of files in the dataset...")

        url: str = self.__session.ddi_path + b64decode(b"L3N0YWdpbmcvbGlzdGluZw====").decode("utf-8")
        post_body: dict[str, str | bool] = {
            "dataset_id": dataset_id
        }

        content: dict = self.__session.make_request("post",
                                                    url,
                                                    json_data=post_body,
                                                    headers=self.__session.get_api_headers(),
                                                    to_json=True)
        
        if content_as_pandas:
            content = convert_dir_tree_to_pandas(content, 
                                                 suppress_print=self._suppress_print)

        if not self._suppress_print:
            print(f"Content of the dataset was successfully retrieved (and converted)...")

        if self._print_content:
            print(f"{content}")
        
        return content
               

    @handle_class_exceptions()
    def get_dataset_path(self, 
                         access: Literal["user", "project", "public"], 
                         project: str, 
                         dataset_id: str, 
                         username: str | None=None) -> str | None:
        """
            Returns a path for an existing dataset as the combination of access, project, internalID and username.

            Parameters:
            -----------
            access : Literal["public", "project", "user"]
                Access mode of the project (user, project, public)
            project : str
                Project's shortname.
            dataset_id : str
                Dataset ID as UUID.
            username : str | None
                The iRODS username. Needed when user access is defined

            Returns:
            --------
            str
                Staging dataset path.

        """

        access = check_access(access=access,
                              log=self._logging)
        
        project = is_string(parameter_name="project",
                            parameter=project,
                            log=self._logging)
        
        dataset_id = check_if_uuid(uuid=dataset_id,
                                   parameter_name="dataset_id",
                                   log=self._logging)
        
        path: str = assemble_dataset_path(access=access, 
                                          project=project, 
                                          dataset_id=dataset_id, 
                                          username=username)

        return path